{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute classifier metrics\n",
    "\n",
    "The snakemake pipeline output the probability of 0 / 1 for each cell for each classifier. Here, we compute and save many common metrics from these probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import plotnine as plotnine\n",
    "import polars as pl\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score,\n",
    "    balanced_accuracy_score,\n",
    "    confusion_matrix,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths\n",
    "pipeline = \"profiles_tcdropped_filtered_var_mad_outlier_featselect_filtcells_metacorr\"\n",
    "snakemake_dir = \"/dgx1nas1/storage/data/jess/repos/2021_09_01_VarChAMP/6.downstream_analysis_snakemake\"\n",
    "res_b7 = f\"{snakemake_dir}/outputs/results/2024_01_23_Batch_7/{pipeline}\"\n",
    "res_b8 = f\"{snakemake_dir}/outputs/results/2024_02_06_Batch_8/{pipeline}\"\n",
    "metrics_dir = \"/dgx1nas1/storage/data/jess/varchamp/sc_data/classification_results/B7B8_1percent_updatedmeta\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in bb classifier info\n",
    "info_b7 = pl.read_csv(f\"{res_b7}/classifier_info.csv\")\n",
    "info_b7 = info_b7.with_columns(\n",
    "    (pl.col(\"trainsize_1\") / (pl.col(\"trainsize_0\") + pl.col(\"trainsize_1\"))).alias(\n",
    "        \"train_prob_1\"\n",
    "    ),\n",
    "    (pl.col(\"testsize_1\") / (pl.col(\"testsize_0\") + pl.col(\"testsize_1\"))).alias(\n",
    "        \"test_prob_1\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "info_b8 = pl.read_csv(f\"{res_b8}/classifier_info.csv\")\n",
    "info_b8 = info_b8.with_columns(\n",
    "    (pl.col(\"trainsize_1\") / (pl.col(\"trainsize_0\") + pl.col(\"trainsize_1\"))).alias(\n",
    "        \"train_prob_1\"\n",
    "    ),\n",
    "    (pl.col(\"testsize_1\") / (pl.col(\"testsize_0\") + pl.col(\"testsize_1\"))).alias(\n",
    "        \"test_prob_1\"\n",
    "    ),\n",
    ")\n",
    "\n",
    "info = pl.concat([info_b7, info_b8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classifier predictions\n",
    "preds_b8 = pl.scan_parquet(f\"{res_b8}/predictions.parquet\")\n",
    "preds_b8 = preds_b8.with_columns(pl.lit(\"batch8\").alias(\"Batch\")).collect()\n",
    "\n",
    "preds_b7 = pl.scan_parquet(f\"{res_b7}/predictions.parquet\")\n",
    "preds_b7 = preds_b7.with_columns(pl.lit(\"batch7\").alias(\"Batch\")).collect()\n",
    "\n",
    "\n",
    "preds = pl.concat([preds_b7, preds_b8]).with_columns(\n",
    "    pl.concat_str(\n",
    "        [pl.col(\"Classifier_ID\"), pl.col(\"Metadata_Protein\"), pl.col(\"Batch\")],\n",
    "        separator=\"_\",\n",
    "    ).alias(\"Full_Classifier_ID\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to compute metrics for each group\n",
    "def compute_aubprc(auprc, prior):\n",
    "    return (auprc * (1 - prior)) / ((auprc * (1 - prior)) + ((1 - auprc) * prior))\n",
    "\n",
    "\n",
    "def compute_metrics(group):\n",
    "    y_true = group[\"Label\"].to_numpy()\n",
    "    y_prob = group[\"Prediction\"].to_numpy()\n",
    "    y_pred = (y_prob > 0.5).astype(int)\n",
    "    prior = sum(y_true == 1) / len(y_true)\n",
    "\n",
    "    class_ID = group[\"Classifier_ID\"].unique()[0]\n",
    "\n",
    "    # Compute AUROC\n",
    "    auroc = roc_auc_score(y_true, y_prob)\n",
    "\n",
    "    # Compute AUPRC\n",
    "    auprc = average_precision_score(y_true, y_prob)\n",
    "    aubprc = compute_aubprc(auprc, prior)\n",
    "\n",
    "    # Compute macro-averaged F1 score\n",
    "    macro_f1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "\n",
    "    # Compute sensitivity and specificity\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    sensitivity = tp / (tp + fn)\n",
    "    specificity = tn / (tn + fp)\n",
    "\n",
    "    # Compute balanced accuracy\n",
    "    balanced_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "\n",
    "    return {\n",
    "        \"AUROC\": auroc,\n",
    "        \"AUPRC\": auprc,\n",
    "        \"AUBPRC\": aubprc,\n",
    "        \"Macro_F1\": macro_f1,\n",
    "        \"Sensitivity\": sensitivity,\n",
    "        \"Specificity\": specificity,\n",
    "        \"Balanced_Accuracy\": balanced_acc,\n",
    "        \"Classifier_ID\": class_ID,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an empty list to store the results\n",
    "results = []\n",
    "classIDs = preds.select(\"Full_Classifier_ID\").to_series().unique().to_list()\n",
    "\n",
    "# Group by Classifier_ID and compute metrics for each group\n",
    "for id in tqdm(classIDs):\n",
    "    metrics = compute_metrics(preds.filter(pl.col(\"Full_Classifier_ID\") == id))\n",
    "    metrics[\"Full_Classifier_ID\"] = id\n",
    "    results.append(metrics)\n",
    "\n",
    "# Convert the results to a Polars DataFrame\n",
    "metrics_df = pl.DataFrame(results)\n",
    "\n",
    "# Add classifier info and save\n",
    "metrics_df = metrics_df.join(info, on=\"Classifier_ID\")\n",
    "metrics_df = metrics_df.with_columns(\n",
    "    (\n",
    "        pl.max_horizontal([\"trainsize_0\", \"trainsize_1\"])\n",
    "        / pl.min_horizontal([\"trainsize_0\", \"trainsize_1\"])\n",
    "    ).alias(\"Training_imbalance\"),\n",
    "    (\n",
    "        pl.max_horizontal([\"testsize_0\", \"testsize_1\"])\n",
    "        / pl.min_horizontal([\"testsize_0\", \"testsize_1\"])\n",
    "    ).alias(\"Testing_imbalance\"),\n",
    ")\n",
    "metrics_df.write_csv(f\"{metrics_dir}/metrics.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "varchamp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
